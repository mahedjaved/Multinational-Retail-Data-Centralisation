{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install SQLAlchemy\n",
    "# !pip install psycopg2\n",
    "# !pip install pandas\n",
    "# !pip install PyYAML\n",
    "# !pip install tabula-py\n",
    "# !pip install requests\n",
    "# !pip install boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine, MetaData, Table, select\n",
    "from dateutil.parser import parse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import psycopg2\n",
    "import yaml\n",
    "import re\n",
    "import boto3\n",
    "import tabula\n",
    "\n",
    "# using the requests library to GET the number of stores\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_weights(weight):\n",
    "    \"\"\"\n",
    "    @desc: check if the input matches the numeric and alphabet matching expression\n",
    "    \"\"\"\n",
    "    match = re.match(r\"([\\d.]+)([a-zA-Z]+)\", weight)\n",
    "    # if there is match then set the first output as value (numeric) and second as unit (g, kg, ml or l)\n",
    "    if match:\n",
    "        value, unit = match.groups()\n",
    "        # conver the numeric value to floating pt\n",
    "        value = float(value)\n",
    "        # check for the cases of 'g', 'ml' and 'l'\n",
    "        if unit == 'g':\n",
    "            value /= 1000\n",
    "            unit = 'kg'\n",
    "        elif unit == 'ml':\n",
    "            value /= 1000\n",
    "            unit = 'kg'\n",
    "        elif unit == 'l':\n",
    "            unit = 'kg'\n",
    "        elif unit == 'oz':\n",
    "            value *= 0.0283495\n",
    "            unit = 'kg'\n",
    "        # force the output to be 3 d.p\n",
    "        return f'{value:.3f}{unit}'\n",
    "    else:\n",
    "        return weight\n",
    "    \n",
    "\n",
    "def mullexp_to_netresult(in_exp):\n",
    "    if 'x' in in_exp:\n",
    "        match = re.match(r'(\\d+)\\s*x\\s*(\\d+)([a-zA-Z]+)', in_exp)\n",
    "        if match:\n",
    "            multiplier = int(match.group(1))\n",
    "            value = int(match.group(2))\n",
    "            unit = match.group(3)\n",
    "            # Perform the multiplication\n",
    "            result = multiplier * value\n",
    "            # Append the result with the unit\n",
    "            return str(result) + unit\n",
    "    else:\n",
    "        return in_exp\n",
    "    \n",
    "def is_alpha(in_str):\n",
    "    \"\"\"\n",
    "    @desc: function to check if the column has alphanumeric entries\n",
    "    \"\"\"\n",
    "    return any(c.isalpha() for c in in_str)\n",
    "\n",
    "def is_alphanumeric(in_str):\n",
    "    \"\"\"\n",
    "    @desc: function to check if the column has alphanumeric entries\n",
    "    \"\"\"\n",
    "    return bool(re.fullmatch(r'^[a-zA-Z0-9_]*$', in_str))\n",
    "\n",
    "def has_yyyy_mm_dd_format(in_str):\n",
    "    \"\"\"\n",
    "    @desc: function to decide if the a column of a data has date format yyyy-mm-dd\n",
    "    \"\"\"\n",
    "    return bool(re.fullmatch(r'\\d{4}-\\d{2}-\\d{2}', in_str))\n",
    "\n",
    "def convert_date_to_yyyy_mm_dd(in_column : pd.core.series.Series):\n",
    "    \"\"\"\n",
    "    @desc: function to set the date column with date format yyyy-mm-dd\n",
    "    \"\"\"\n",
    "    in_column = in_column.apply(parse)\n",
    "    in_column = pd.to_datetime(in_column, infer_datetime_format=True, errors='coerce')\n",
    "    \n",
    "    return in_column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milestone 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 -- Users Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test code for Psycopg2\n",
    "# conn = psycopg2.connect(\n",
    "#     host=ENDPOINT,\n",
    "#     port=PORT,\n",
    "#     database=DATABASE,\n",
    "#     user=USER,\n",
    "#     password=PASSWORD\n",
    "# )\n",
    "yaml_file_path = '../db_creds.yaml'\n",
    "\n",
    "# Read the YAML file and store its contents in a Python data structure (dictionary)\n",
    "with open(yaml_file_path, 'r') as file:\n",
    "    yaml_data = yaml.safe_load(file)\n",
    "\n",
    "# credentials\n",
    "DATABASE_TYPE = 'postgresql'\n",
    "# DBAPI = 'psycopg2'\n",
    "ENDPOINT = yaml_data['RDS_HOST']\n",
    "USER = yaml_data['RDS_USER']\n",
    "PASSWORD = yaml_data['RDS_PASSWORD']\n",
    "PORT = yaml_data['RDS_PORT']\n",
    "DATABASE = yaml_data['RDS_DATABASE']\n",
    "\n",
    "# setup sql engine and connect\n",
    "sql_engine = create_engine(f\"{DATABASE_TYPE}://{USER}:{PASSWORD}@{ENDPOINT}:{PORT}/{DATABASE}\")\n",
    "sql_connection = sql_engine.connect()\n",
    "\n",
    "# metadata, holds collection of table info, their data types, schema names etc., obtained from here : https://docs.sqlalchemy.org/en/20/core/metadata.html\n",
    "# pros of MetaData() includes thread safety -> meaning it can handle concurrent tasks from multiple thread (computationally efficient when multiple threads need access to same resource)\n",
    "metadata = MetaData()\n",
    "metadata.reflect(sql_engine)\n",
    "table_names = metadata.tables.keys()\n",
    "\n",
    "# reflect allows us\n",
    "names_ = list(table_names)\n",
    "\n",
    "# read table\n",
    "users_table = sql_connection.execute(select(Table('legacy_users', metadata, autoload=True, autoload_with=sql_engine)))\n",
    "headers = users_table.keys()\n",
    "users_df = pd.DataFrame(users_table.fetchall(), columns=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = 'legacy_users'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for nulls or NaNs, alternative is np.unique(users_df.isnull()), or just use df.info()\n",
    "if users_df.isnull().sum().sum() and users_df.isna().sum().sum():\n",
    "    raise f\"The database : {table_name}, has total {users_df.isnull().sum().sum()} NULL values and {users_df.isna().sum().sum()} NaN values\"\n",
    "else:\n",
    "    print(f\"[usrmsg] No NULLs or NaNs found in {table_name}\")\n",
    "\n",
    "users_df_processed = users_df[~users_df.apply(lambda row: row.astype(str).str.contains('NULL').any(), axis=1)]\n",
    "\n",
    "# check for data types \n",
    "#   -1) always begin with dropping duplicates and storing as a seperate file\n",
    "users_df_processed = users_df_processed.drop_duplicates()\n",
    "#   -2) set all columns except index to be of string format\n",
    "str_convert_dict = {col: 'string' for col in users_df_processed.columns if col not in ['index']}\n",
    "users_df_processed = users_df_processed.astype(str_convert_dict)\n",
    "#   -3) remove all entries that are pure alphanumeric\n",
    "users_df_processed = users_df_processed[~users_df_processed['email_address'].apply(is_alphanumeric)]\n",
    "#   -4) DoB and join_date should be datetime format and of type yyyy-mm-dd\n",
    "users_df_processed['date_of_birth'] = convert_date_to_yyyy_mm_dd(users_df_processed['date_of_birth'])\n",
    "users_df_processed['join_date'] = convert_date_to_yyyy_mm_dd(users_df_processed['join_date'])\n",
    "#   -5) convert all 'GGB' country code to 'GB'\n",
    "users_df_processed['country_code'] = users_df_processed['country_code'].str.replace('GGB', 'GB', regex=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_df_processed.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_file_path = '../local_db_creds.yaml'\n",
    "\n",
    "# Read the YAML file and store its contents ain a Python data structure (dictionary)\n",
    "with open(yaml_file_path, 'r') as file:\n",
    "    yaml_data = yaml.safe_load(file)\n",
    "\n",
    "# credentials\n",
    "DATABASE_TYPE = 'postgresql+psycopg2'\n",
    "ENDPOINT = yaml_data['LOCAL_HOST']\n",
    "USER = yaml_data['LOCAL_USER']\n",
    "PASSWORD = yaml_data['LOCAL_PASSWORD']\n",
    "PORT = yaml_data['LOCAL_PORT']\n",
    "DATABASE = yaml_data['LOCAL_DATABASE']\n",
    "\n",
    "engine = create_engine(f\"{DATABASE_TYPE}://{USER}:{PASSWORD}@{ENDPOINT}:{PORT}/{DATABASE}\")\n",
    "connection = engine.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_df_processed.to_sql(name=\"dim_users\", con=engine, if_exists='replace', index=False, schema='public')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4 -- Card Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the .pdf file\n",
    "pdf_path = \"../card_details.pdf\"\n",
    "card_df_list = tabula.read_pdf(pdf_path, stream=True, pages='all')\n",
    "card_df = pd.concat(card_df_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   -1) drop columns that are filled with missing and/or incorrect information\n",
    "card_processed_df = card_df.drop(columns=['Unnamed: 0'])\n",
    " #  -2) always begin with dropping duplicates \n",
    "card_processed_df = card_processed_df.drop_duplicates()\n",
    "#   -3) remove columns with \"NULL\"\n",
    "card_processed_df = card_processed_df[~card_processed_df.apply(lambda row: row.astype(str).str.contains('NULL').any(), axis=1)]\n",
    "#   -4) remove all entries that are pure alphanumeric\n",
    "card_processed_df = card_processed_df[~card_processed_df['card_provider'].apply(is_alphanumeric)]\n",
    "#   -5) those rows that have NaN in card_number expiry_date, fill those appropriately\n",
    "nan_card_num_expiry_date_df = card_processed_df[card_processed_df['card_number expiry_date'].isna()]\n",
    "nan_card_num_expiry_date_df['card_number expiry_date'] = nan_card_num_expiry_date_df['card_number'].astype(str) + ' ' + nan_card_num_expiry_date_df['expiry_date'].astype(str)\n",
    "#   -6) those rows that DONT have NaN in card_number expiry_date column, strip those isolated and replace their equivalent NaN values in the card_number and the expiry_date columns appropriately\n",
    "not_nan_card_num_expiry_date_df = card_processed_df[~card_processed_df['card_number expiry_date'].isna()]\n",
    "splitted_cardnumexpdate_df = not_nan_card_num_expiry_date_df['card_number expiry_date'].str.split(n=1, expand=True)\n",
    "not_nan_card_num_expiry_date_df['card_number'], not_nan_card_num_expiry_date_df['expiry_date'] = splitted_cardnumexpdate_df[0], splitted_cardnumexpdate_df[1]\n",
    "#   -7) combine the two to store the seperate data with no NaNs . . . (hopefully :P)\n",
    "card_processed_df = pd.concat([nan_card_num_expiry_date_df, not_nan_card_num_expiry_date_df], ignore_index=True)\n",
    "del nan_card_num_expiry_date_df, not_nan_card_num_expiry_date_df\n",
    "#   -8) change all objects to string\n",
    "card_processed_df = card_processed_df.astype('string')\n",
    "#   -9) finally, change all date columns to datetimeformat\n",
    "card_processed_df['date_payment_confirmed'] = convert_date_to_yyyy_mm_dd(card_processed_df['date_payment_confirmed'])\n",
    "card_processed_df['expiry_date'] = pd.to_datetime(card_processed_df['expiry_date'], format='%m/%y') + pd.offsets.MonthEnd(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_file_path = '../local_db_creds.yaml'\n",
    "\n",
    "# Read the YAML file and store its contents in a Python data structure (dictionary)\n",
    "with open(yaml_file_path, 'r') as file:\n",
    "    yaml_data = yaml.safe_load(file)\n",
    "\n",
    "# credentials\n",
    "DATABASE_TYPE = 'postgresql+psycopg2'\n",
    "ENDPOINT = yaml_data['LOCAL_HOST']\n",
    "USER = yaml_data['LOCAL_USER']\n",
    "PASSWORD = yaml_data['LOCAL_PASSWORD']\n",
    "PORT = yaml_data['LOCAL_PORT']\n",
    "DATABASE = yaml_data['LOCAL_DATABASE']\n",
    "\n",
    "engine = create_engine(f\"{DATABASE_TYPE}://{USER}:{PASSWORD}@{ENDPOINT}:{PORT}/{DATABASE}\")\n",
    "connection = engine.connect()\n",
    "card_processed_df.to_sql(\"dim_card_details\", engine, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5 -- Store Related Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_file_path = '../api_key.yaml'\n",
    "\n",
    "# Read the YAML file and store its contents in a Python data structure (dictionary)\n",
    "with open(yaml_file_path, 'r') as file:\n",
    "    yaml_data = yaml.safe_load(file)\n",
    "\n",
    "\n",
    "# Retrieve a store: https://aqj7u5id95.execute-api.eu-west-1.amazonaws.com/prod/store_details/{store_number}\n",
    "# Return the number of stores: https://aqj7u5id95.execute-api.eu-west-1.amazonaws.com/prod/number_stores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Common methods include `GET` (retrieve resource), `POST` (create resource), `PUT` (update resource), and `DELETE` (remove resource). We will discuss in detail about them later\n",
    "\n",
    "- **Status Codes**: HTTP status codes indicate the result of the request. These codes range from informational (`1xx`) to success (`2xx`), redirection (`3xx`), client errors (`4xx`), and server errors (`5xx`).   \n",
    "\n",
    "- **Port**: HTTPS typically uses port 443 for communication, while HTTP uses port 80. The use of a different port helps differentiate between secure and non-secure connections.\n",
    "\n",
    "- The request contains information such as the `HTTP method`, `the endpoint URL`, optional `headers`, and, in some cases, a `payload` or `body`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **HTTP Method**: The HTTP method, also known as the HTTP verb, specifies the action to be performed on the resource. Common HTTP methods include `GET` (retrieve data), `POST` (create data), `PUT` (update data), and `DELETE` (remove data).\n",
    "\n",
    "- **Endpoint URL**: The **endpoint URL** represents the specific resource or functionality on the server that the client wants to interact with. It typically follows a specific URL pattern defined by the API.\n",
    "\n",
    "- **Headers**: Headers provide additional information about the request, such as content type, authorization tokens, or caching directives. We will learn more about headers  in a later lesson.\n",
    "\n",
    "- **Payload or Body**: In certain cases, requests may include a payload or body `containing data to be sent to the server`. This is common for methods like `POST` or `PUT`, where the payload contains the data to create or update a resource."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Endpoint URLs have the usual format of  . . .\n",
    "```\n",
    "<ROOT_URL>/<Path>?<Query Parameters>\n",
    "```\n",
    "\n",
    "In this structure:\n",
    "- `<ROOT_URL>` represents the base URL of the API\n",
    "- `<Path>` refers to the specific path or endpoint within the API that offers a specific service\n",
    "- `<Query Parameters>` are optional parameters passed in the URL query string, allowing for additional customization or filtering of the request\n",
    "\n",
    "After the path section of the endpoint URL, one or more *parameters* can be specified\n",
    "\n",
    "* The `?` symbol denotes the separation between the endpoint path and the start of the parameters\n",
    "* while multiple parameters are typically separated using the `&` symbol.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_number = 450\n",
    "store_detail = []\n",
    "\n",
    "get_all_stores_url = \"https://aqj7u5id95.execute-api.eu-west-1.amazonaws.com/prod/number_stores\"\n",
    "headers = {\n",
    "    \"X-API-KEY\": yaml_data[\"API_KEY\"]\n",
    "}\n",
    "\n",
    "for i in range(store_number):\n",
    "    get_store_number_url = f\"https://aqj7u5id95.execute-api.eu-west-1.amazonaws.com/prod/store_details/{i}\"\n",
    "    response = requests.get(get_store_number_url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        # Access the response data as JSON\n",
    "        store_detail.append(response.json())\n",
    "\n",
    "    else:\n",
    "        print(f\"Request failed with status code: {response.status_code}\")\n",
    "        print(f\"Response Text: {response.text}\")\n",
    "assert len(store_detail) == 450"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_detail_df = pd.DataFrame(store_detail)\n",
    "store_detail_df_copy = store_detail_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   -1) remove purely nan or none columns (e.g. lat)\n",
    "store_detail_df_copy = store_detail_df_copy.drop(columns=\"lat\")\n",
    "store_detail_df_copy = store_detail_df_copy.drop(columns=\"address\")\n",
    "#   -2) remove all pure alphanmueric rows\n",
    "no_alphanum_df = store_detail_df_copy[~store_detail_df_copy['opening_date'].apply(is_alphanumeric)]\n",
    "#   -3) account for missing addresses, longitude and latitude values\n",
    "# --> ANS) No need to change, it is a portal type store, and only one and unique in the table\n",
    "#   -4) remove all alphabets in staff_numbers column\n",
    "no_alphanum_df[\"staff_numbers\"] = no_alphanum_df[\"staff_numbers\"].str.replace(r'[a-zA-Z]', '', regex=True)\n",
    "#   -5) fix format of opening_date\n",
    "no_alphanum_df[\"opening_date\"] = convert_date_to_yyyy_mm_dd(no_alphanum_df[\"opening_date\"])\n",
    "#   -6) set eeEurope and eeAmerica to Europe and America in the continent column\n",
    "no_alphanum_df[\"continent\"] = no_alphanum_df[\"continent\"].str.replace('eeEurope', 'Europe')\n",
    "no_alphanum_df[\"continent\"] = no_alphanum_df[\"continent\"].str.replace('eeAmerica', 'America')\n",
    "#   -7) convert all object to string appropriately and all numbers to int and float appropriately\n",
    "no_alphanum_df = no_alphanum_df.astype({col: 'string' for col in no_alphanum_df.columns if col not in [\"index\", \"opening_date\", \"longitude\", \"staff_numbers\", \"latitude\"]})\n",
    "# no_alphanum_df = no_alphanum_df.astype({col: 'float64' for col in no_alphanum_df.columns if col in [\"longitude\", \"latitude\"]})\n",
    "no_alphanum_df[\"longitude\"] = pd.to_numeric(no_alphanum_df[\"longitude\"], errors='coerce')\n",
    "no_alphanum_df[\"latitude\"] = pd.to_numeric(no_alphanum_df[\"latitude\"], errors='coerce')\n",
    "no_alphanum_df[\"staff_numbers\"] = pd.to_numeric(no_alphanum_df[\"staff_numbers\"], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_file_path = '../local_db_creds.yaml'\n",
    "\n",
    "# Read the YAML file and store its contents in a Python data structure (dictionary)\n",
    "with open(yaml_file_path, 'r') as file:\n",
    "    yaml_data = yaml.safe_load(file)\n",
    "\n",
    "# credentials\n",
    "DATABASE_TYPE = 'postgresql+psycopg2'\n",
    "ENDPOINT = yaml_data['LOCAL_HOST']\n",
    "USER = yaml_data['LOCAL_USER']\n",
    "PASSWORD = yaml_data['LOCAL_PASSWORD']\n",
    "PORT = yaml_data['LOCAL_PORT']\n",
    "DATABASE = yaml_data['LOCAL_DATABASE']\n",
    "\n",
    "engine = create_engine(f\"{DATABASE_TYPE}://{USER}:{PASSWORD}@{ENDPOINT}:{PORT}/{DATABASE}\")\n",
    "connection = engine.connect()\n",
    "no_alphanum_df.to_sql(\"dim_store_details\", engine, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6 -- Products Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* There is a nan in `category`\n",
    "* There are alphanumerics in `category`\n",
    "* The first alphabets in `product_code` need to be capitalised\n",
    "* nan and alphanumerics in `product_price` and `removed`\n",
    "\n",
    "> special instructions for weight (keep last)\n",
    "* there are some calculations that need to be done\n",
    "* 'g' need to be converted to 'kg'\n",
    "* all numbers should have consistent dp of 3dp\n",
    "\n",
    "`course instructions`\n",
    "* Convert them all to a decimal value representing their weight in kg. Use a 1:1 ratio of ml to g as a rough estimate for the rows containing ml.\n",
    "* Develop the method to clean up the weight column and remove all excess characters then represent the weights as a float."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s3 = boto3.client('s3')\n",
    "# s3.download_file('data-handling-public', 'products.csv', '../products_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "products_org_df = pd.read_csv('../products_data.csv', )\n",
    "products_org_copy_df = products_org_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   -1) always being by dropping duplicates\n",
    "products_org_copy_df = products_org_copy_df.drop_duplicates()\n",
    "#   -2) rename the Unamed column to index\n",
    "products_org_copy_df.rename(columns={'Unnamed: 0': 'index'}, inplace=True)\n",
    "#   -3) fill nans with empty strings to allow easier processing for the rest of the cleaning\n",
    "products_org_copy_df = products_org_copy_df.fillna('')\n",
    "#   -4) remove all the pure alphanumeric entries\n",
    "products_org_copy_df = products_org_copy_df[~products_org_copy_df['date_added'].apply(is_alphanumeric)]\n",
    "#   -5) for weights : a) compute all multiplication expressions and replace with resultant value\n",
    "products_org_copy_df[\"weight\"] = products_org_copy_df[\"weight\"].apply(mullexp_to_netresult)\n",
    "#   -6) for weights : b) standardise them to 'kg'\n",
    "products_org_copy_df[\"weight\"] = products_org_copy_df[\"weight\"].apply(convert_weights)\n",
    "#   -7) drop £ and kg to enable weight and product price columns to be numeric\n",
    "products_org_copy_df[\"product_price\"] = products_org_copy_df[\"product_price\"].str.replace('£', '')\n",
    "products_org_copy_df.rename(columns={'product_price': 'product_price (£)'}, inplace=True)\n",
    "products_org_copy_df[\"weight\"] = products_org_copy_df[\"weight\"].str.replace('kg', '')\n",
    "products_org_copy_df.rename(columns={'weight': 'weight (kg)'}, inplace=True)\n",
    "#   -8) set product_price, weight, EAN to be numeric\n",
    "products_org_copy_df[\"product_price (£)\"] = pd.to_numeric(products_org_copy_df[\"product_price (£)\"], errors='coerce')\n",
    "products_org_copy_df[\"weight (kg)\"] = pd.to_numeric(products_org_copy_df[\"weight (kg)\"], errors='coerce')\n",
    "products_org_copy_df[\"EAN\"] = pd.to_numeric(products_org_copy_df[\"EAN\"], errors='coerce')\n",
    "#   -9) set product_name, cateogry, uuid, removed and product_code to string  AND  date_added to datetime\n",
    "products_org_copy_df = products_org_copy_df.astype({\"product_name\" : \"string\", \"category\" : \"string\", \"uuid\" : \"string\", \"removed\" : \"string\", \"product_code\" : \"string\"})\n",
    "products_org_copy_df['date_added'] = convert_date_to_yyyy_mm_dd(products_org_copy_df['date_added'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_file_path = '../local_db_creds.yaml'\n",
    "\n",
    "# Read the YAML file and store its contents in a Python data structure (dictionary)\n",
    "with open(yaml_file_path, 'r') as file:\n",
    "    yaml_data = yaml.safe_load(file)\n",
    "\n",
    "# credentials\n",
    "DATABASE_TYPE = 'postgresql+psycopg2'\n",
    "ENDPOINT = yaml_data['LOCAL_HOST']\n",
    "USER = yaml_data['LOCAL_USER']\n",
    "PASSWORD = yaml_data['LOCAL_PASSWORD']\n",
    "PORT = yaml_data['LOCAL_PORT']\n",
    "DATABASE = yaml_data['LOCAL_DATABASE']\n",
    "\n",
    "engine = create_engine(f\"{DATABASE_TYPE}://{USER}:{PASSWORD}@{ENDPOINT}:{PORT}/{DATABASE}\")\n",
    "connection = engine.connect()\n",
    "products_org_copy_df.to_sql(\"dim_products\", engine, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 7 -- Orders Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_file_path = '../db_creds.yaml'\n",
    "\n",
    "# Read the YAML file and store its contents in a Python data structure (dictionary)\n",
    "with open(yaml_file_path, 'r') as file:\n",
    "    yaml_data = yaml.safe_load(file)\n",
    "\n",
    "# credentials\n",
    "DATABASE_TYPE = 'postgresql'\n",
    "# DBAPI = 'psycopg2'\n",
    "ENDPOINT = yaml_data['RDS_HOST']\n",
    "USER = yaml_data['RDS_USER']\n",
    "PASSWORD = yaml_data['RDS_PASSWORD']\n",
    "PORT = yaml_data['RDS_PORT']\n",
    "DATABASE = yaml_data['RDS_DATABASE']\n",
    "\n",
    "# setup sql engine and connect\n",
    "sql_engine = create_engine(f\"{DATABASE_TYPE}://{USER}:{PASSWORD}@{ENDPOINT}:{PORT}/{DATABASE}\")\n",
    "sql_connection = sql_engine.connect()\n",
    "\n",
    "# metadata, holds collection of table info, their data types, schema names etc., obtained from here : https://docs.sqlalchemy.org/en/20/core/metadata.html\n",
    "# pros of MetaData() includes thread safety -> meaning it can handle concurrent tasks from multiple thread (computationally efficient when multiple threads need access to same resource)\n",
    "metadata = MetaData()\n",
    "metadata.reflect(sql_engine)\n",
    "table_names = metadata.tables.keys()\n",
    "\n",
    "# reflect allows us\n",
    "names_ = list(table_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read table\n",
    "orders_table = sql_connection.execute(select(Table('orders_table', metadata, autoload=True, autoload_with=sql_engine)))\n",
    "headers = orders_table.keys()\n",
    "orders_org_df = pd.DataFrame(orders_table.fetchall(), columns=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_processed_df = orders_org_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   -1) remove columns first_name, last_name and 1\n",
    "order_processed_df = order_processed_df.drop(columns={'first_name', 'last_name', '1'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_file_path = '../local_db_creds.yaml'\n",
    "\n",
    "# Read the YAML file and store its contents in a Python data structure (dictionary)\n",
    "with open(yaml_file_path, 'r') as file:\n",
    "    yaml_data = yaml.safe_load(file)\n",
    "\n",
    "# credentials\n",
    "DATABASE_TYPE = 'postgresql+psycopg2'\n",
    "ENDPOINT = yaml_data['LOCAL_HOST']\n",
    "USER = yaml_data['LOCAL_USER']\n",
    "PASSWORD = yaml_data['LOCAL_PASSWORD']\n",
    "PORT = yaml_data['LOCAL_PORT']\n",
    "DATABASE = yaml_data['LOCAL_DATABASE']\n",
    "\n",
    "engine = create_engine(f\"{DATABASE_TYPE}://{USER}:{PASSWORD}@{ENDPOINT}:{PORT}/{DATABASE}\")\n",
    "connection = engine.connect()\n",
    "order_processed_df.to_sql(\"orders_table\", engine, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 8 -- Date Events Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://data-handling-public.s3.eu-west-1.amazonaws.com/date_details.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client('s3')\n",
    "s3.download_file('data-handling-public', 'date_details.json', '../date_details.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_df = pd.read_json('../date_details.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   -1) drop duplicates and store a copy of the original\n",
    "events_df_processed = events_df.copy().drop_duplicates()\n",
    "#   -2) remove all entries that are purely alphanumeric in nature\n",
    "events_df_processed = events_df_processed[~events_df_processed[\"date_uuid\"].apply(is_alphanumeric)]\n",
    "#   -3) use info from year, month, day and timestamp to set a seperate datetime column\n",
    "events_df_processed['datetime'] = pd.to_datetime(events_df_processed[['year', 'month', 'day', 'timestamp']].astype(str).agg(' '.join, axis=1), format='%Y %m %d %H:%M:%S')\n",
    "#   -4) set timestamp, timeperiod and date_uuid as string\n",
    "events_df_processed = events_df_processed.astype({\"timestamp\" : \"string\", \"time_period\" : \"string\", \"date_uuid\" : \"string\"})\n",
    "#   -5) set month, year and day as int32\n",
    "events_df_processed[\"month\"] = pd.to_numeric(events_df_processed[\"month\"], errors='coerce')\n",
    "events_df_processed[\"year\"] = pd.to_numeric(events_df_processed[\"year\"], errors='coerce')\n",
    "events_df_processed[\"day\"] = pd.to_numeric(events_df_processed[\"day\"], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_file_path = '../local_db_creds.yaml'\n",
    "\n",
    "# Read the YAML file and store its contents in a Python data structure (dictionary)\n",
    "with open(yaml_file_path, 'r') as file:\n",
    "    yaml_data = yaml.safe_load(file)\n",
    "\n",
    "# credentials\n",
    "DATABASE_TYPE = 'postgresql+psycopg2'\n",
    "ENDPOINT = yaml_data['LOCAL_HOST']\n",
    "USER = yaml_data['LOCAL_USER']\n",
    "PASSWORD = yaml_data['LOCAL_PASSWORD']\n",
    "PORT = yaml_data['LOCAL_PORT']\n",
    "DATABASE = yaml_data['LOCAL_DATABASE']\n",
    "\n",
    "engine = create_engine(f\"{DATABASE_TYPE}://{USER}:{PASSWORD}@{ENDPOINT}:{PORT}/{DATABASE}\")\n",
    "connection = engine.connect()\n",
    "events_df_processed.to_sql(\"dim_date_times\", engine, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finalizing Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database Connector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "@reqs:\n",
    "    [1] PyYAML : !pip install PyYAML\n",
    "    [2] psycopg : !pip install psycopg2\n",
    "\"\"\"\n",
    "from sqlalchemy import create_engine, MetaData\n",
    "import pandas as pd\n",
    "import yaml\n",
    "\n",
    "class DatabaseConnector:\n",
    "    \"\"\"\n",
    "    @desc : use to connect with and upload data to the database.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    \n",
    "    def read_db_creds(self):\n",
    "        \"\"\"\n",
    "        @desc: reqs-> install PyYAML: pip install PyYAML\n",
    "        \"\"\"\n",
    "        yaml_file_path = '../db_creds.yaml'\n",
    "\n",
    "        # Read the YAML file and store its contents in a Python data structure (dictionary)\n",
    "        with open(yaml_file_path, 'r') as file:\n",
    "            return yaml.safe_load(file)\n",
    "        \n",
    "\n",
    "    def init_db_engine(self):\n",
    "        yaml_data = self.read_db_creds()\n",
    "        DATABASE_TYPE = 'postgresql'\n",
    "        DBAPI = 'psycopg2'\n",
    "        ENDPOINT = yaml_data['RDS_HOST']\n",
    "        USER = yaml_data['RDS_USER']\n",
    "        PASSWORD = yaml_data['RDS_PASSWORD']\n",
    "        PORT = yaml_data['RDS_PORT']\n",
    "        DATABASE = yaml_data['RDS_DATABASE']\n",
    "\n",
    "        return create_engine(f\"{DATABASE_TYPE}://{USER}:{PASSWORD}@{ENDPOINT}:{PORT}/{DATABASE}\")\n",
    "    \n",
    "    def list_db_tables(self):\n",
    "        # setup sql engine and connect\n",
    "        sql_engine = self.init_db_engine()\n",
    "        sql_engine.connect()\n",
    "\n",
    "        # metadata, holds collection of table info, their data types, names \n",
    "        metadata = MetaData()\n",
    "        metadata.reflect(sql_engine)\n",
    "        table_names = metadata.tables.keys()\n",
    "\n",
    "        # return the table names in a list format\n",
    "        return list(table_names)\n",
    "    \n",
    "\n",
    "    def upload_to_db(self, in_df : pd.DataFrame, table_name : str):\n",
    "        # Read the YAML file and store its contents in a Python data structure (dictionary)\n",
    "        yaml_file_path = '../local_db_creds.yaml'\n",
    "        with open(yaml_file_path, 'r') as file:\n",
    "            yaml_data = yaml.safe_load(file)\n",
    "        \n",
    "        # setup credentials\n",
    "        DATABASE_TYPE = 'postgresql+psycopg2'\n",
    "        ENDPOINT = yaml_data['LOCAL_HOST']\n",
    "        USER = yaml_data['LOCAL_USER']\n",
    "        PASSWORD = yaml_data['LOCAL_PASSWORD']\n",
    "        PORT = yaml_data['LOCAL_PORT']\n",
    "        DATABASE = yaml_data['LOCAL_DATABASE']\n",
    "\n",
    "        # connect and upload, index=False iff there exists a seperate column for index\n",
    "        engine = create_engine(f\"{DATABASE_TYPE}://{USER}:{PASSWORD}@{ENDPOINT}:{PORT}/{DATABASE}\")\n",
    "        connection = engine.connect()\n",
    "        in_df.to_sql(table_name, engine, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine, MetaData, Table, select\n",
    "import pandas as pd\n",
    "import requests\n",
    "import tabula\n",
    "import boto3\n",
    "import yaml\n",
    "\n",
    "class DataExtractor(DatabaseConnector):\n",
    "    \"\"\"\n",
    "    @desc : This class will work as a utility class, in it you will be creating methods that help extract data from different data sources.\n",
    "    \n",
    "    The methods contained will be fit to extract data from a particular data source, these sources will include CSV files, an API and an S3 bucket.\n",
    "    \"\"\"\n",
    "    def read_rds_table(super, table_name='legacy_users'):\n",
    "        \"\"\"\n",
    "        @desc:  extract the database table to a pandas DataFrame.\n",
    "        @inputs: \n",
    "            [1] table_name : the table that needs to be inspected\n",
    "        \"\"\"\n",
    "        sql_engine = super.init_db_engine()\n",
    "        sql_connection = sql_engine.connect()\n",
    "        metadata = MetaData().reflect(sql_engine)\n",
    "        users_table = Table(table_name, metadata, autoload=True, autoload_with=sql_engine)\n",
    "    \n",
    "        return pd.DataFrame(sql_connection.execute(select(users_table)).fetchall())\n",
    "    \n",
    "    def retrieve_pdf_data(self, link2pdf : str):\n",
    "        \"\"\"\n",
    "        @desc: given the link to the pdf document, this function will return the pd.Dataframe of that doc\n",
    "        \"\"\"\n",
    "        return pd.concat(tabula.read_pdf(link2pdf, stream=True, pages='all'), ignore_index=True)\n",
    "    \n",
    "    def list_number_of_stores(self, header : dict, endpoint : str = \"https://aqj7u5id95.execute-api.eu-west-1.amazonaws.com/prod/number_stores\"):\n",
    "        \"\"\"\n",
    "        @desc: retrieves the store numbers data from the stores API, based on the input header and endpoint\n",
    "        \"\"\"\n",
    "        response = requests.get(endpoint, headers=header)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            # Access the response data as JSON\n",
    "            data = response.json()\n",
    "\n",
    "            # Extract and print the name of the Pokémon\n",
    "            num_stores = data['number_stores']\n",
    "            print(f\"Number of Stores: {num_stores}\")\n",
    "            return num_stores\n",
    "\n",
    "        else:\n",
    "            print(f\"Request failed with status code: {response.status_code}\")\n",
    "            print(f\"Response Text: {response.text}\")\n",
    "\n",
    "\n",
    "    def retrieve_stores_data(self, retrieve_store_endpoint : str = \"https://aqj7u5id95.execute-api.eu-west-1.amazonaws.com/prod/store_details/\"):\n",
    "        \"\"\"\n",
    "        @desc: given the retrieve a store endpoint, this function will return the data from all the stores as a pd.Dateframe\n",
    "        \"\"\"\n",
    "        # the total stores information and a list to hold all of the responses\n",
    "        store_number = 450\n",
    "        store_detail = []\n",
    "        \n",
    "        # Read the YAML file and store its contents in a Python data structure (dictionary)\n",
    "        yaml_file_path = '../api_key.yaml'\n",
    "        with open(yaml_file_path, 'r') as file:\n",
    "            yaml_data = yaml.safe_load(file)\n",
    "        \n",
    "        # setup the API key header\n",
    "        headers = {\n",
    "            \"X-API-KEY\": yaml_data[\"API_KEY\"]\n",
    "        }\n",
    "\n",
    "        # loop through the store numbers and store the detail of the individual stores\n",
    "        for i in range(store_number):\n",
    "            url = retrieve_store_endpoint + str(i)\n",
    "            response = requests.get(url, headers=headers)\n",
    "            if response.status_code == 200:\n",
    "                store_detail.append(response.json())\n",
    "\n",
    "            else:\n",
    "                print(f\"Request failed with status code: {response.status_code}\")\n",
    "                print(f\"Response Text: {response.text}\")\n",
    "        assert len(store_detail) == 450\n",
    "        return pd.DataFrame(store_detail)\n",
    "    \n",
    "\n",
    "    def extract_from_s3(self):\n",
    "        \"\"\"\n",
    "        @desc: retrives the products.csv table from the S3 bucket at s3://data-handling-public/products.csv\n",
    "        \"\"\"\n",
    "        s3 = boto3.client('s3')\n",
    "        s3.download_file('data-handling-public', 'products.csv', '../products_data.csv')\n",
    "        return pd.read_csv('../products_data.csv') \n",
    "    \n",
    "\n",
    "    def extract_eventstable_from_s3(self):\n",
    "        \"\"\"\n",
    "        @desc: retrives the date_details.json table from the S3 bucket at https://data-handling-public.s3.eu-west-1.amazonaws.com/date_details.json\n",
    "        \"\"\"\n",
    "        s3 = boto3.client('s3')\n",
    "        s3.download_file('data-handling-public', 'date_details.json', '../date_details.json')\n",
    "        events_df = pd.read_json('../date_details.json')\n",
    "\n",
    "        #   -1) drop duplicates and store a copy of the original\n",
    "        events_df_processed = events_df.copy().drop_duplicates()\n",
    "        #   -2) remove all entries that are purely alphanumeric in nature\n",
    "        events_df_processed = events_df_processed[~events_df_processed[\"date_uuid\"].apply(is_alphanumeric)]\n",
    "        #   -3) use info from year, month, day and timestamp to set a seperate datetime column\n",
    "        events_df_processed['datetime'] = pd.to_datetime(events_df_processed[['year', 'month', 'day', 'timestamp']].astype(str).agg(' '.join, axis=1), format='%Y %m %d %H:%M:%S')\n",
    "        #   -4) set timestamp, timeperiod and date_uuid as string\n",
    "        events_df_processed = events_df_processed.astype({\"timestamp\" : \"string\", \"time_period\" : \"string\", \"date_uuid\" : \"string\"})\n",
    "        #   -5) set month, year and day as int64\n",
    "        events_df_processed[\"month\"] = pd.to_numeric(events_df_processed[\"month\"], errors='coerce')\n",
    "        events_df_processed[\"year\"] = pd.to_numeric(events_df_processed[\"year\"], errors='coerce')\n",
    "        events_df_processed[\"day\"] = pd.to_numeric(events_df_processed[\"day\"], errors='coerce')\n",
    "\n",
    "        return events_df_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from dateutil.parser import parse\n",
    "class DataCleaning:\n",
    "    \"\"\"\n",
    "    @desc : methods to clean data from each of the data sources\n",
    "    \"\"\"\n",
    "    \n",
    "    # ================================================================== #\n",
    "    #                   UTILITY FUNCTIONS\n",
    "    # ================================================================== #\n",
    "    @staticmethod\n",
    "    def convert_weights(weight):\n",
    "        \"\"\"\n",
    "        @desc: check if the input matches the numeric and alphabet matching expression\n",
    "        \"\"\"\n",
    "        match = re.match(r\"([\\d.]+)([a-zA-Z]+)\", weight)\n",
    "        # if there is match then set the first output as value (numeric) and second as unit (g, kg, ml or l)\n",
    "        if match:\n",
    "            value, unit = match.groups()\n",
    "            # conver the numeric value to floating pt\n",
    "            value = float(value)\n",
    "            # check for the cases of 'g', 'ml' and 'l'\n",
    "            if unit == 'g':\n",
    "                value /= 1000\n",
    "                unit = 'kg'\n",
    "            elif unit == 'ml':\n",
    "                value /= 1000\n",
    "                unit = 'kg'\n",
    "            elif unit == 'l':\n",
    "                unit = 'kg'\n",
    "            elif unit == 'oz':\n",
    "                value *= 0.0283495\n",
    "                unit = 'kg'\n",
    "            # force the output to be 3 d.p\n",
    "            return f'{value:.3f}{unit}'\n",
    "        else:\n",
    "            return weight\n",
    "    \n",
    "    @staticmethod\n",
    "    def mullexp_to_netresult(in_exp):\n",
    "        if 'x' in in_exp:\n",
    "            match = re.match(r'(\\d+)\\s*x\\s*(\\d+)([a-zA-Z]+)', in_exp)\n",
    "            if match:\n",
    "                multiplier = int(match.group(1))\n",
    "                value = int(match.group(2))\n",
    "                unit = match.group(3)\n",
    "                # Perform the multiplication\n",
    "                result = multiplier * value\n",
    "                # Append the result with the unit\n",
    "                return str(result) + unit\n",
    "        else:\n",
    "            return in_exp\n",
    "        \n",
    "\n",
    "    @staticmethod\n",
    "    def is_alpha(in_str):\n",
    "        \"\"\"\n",
    "        @desc: function to check if the column has ONLY alphabets entries\n",
    "        \"\"\"\n",
    "        return any(c.isalpha() for c in in_str)\n",
    "    \n",
    "    @staticmethod\n",
    "    def is_alphanumeric(in_str):\n",
    "        \"\"\"\n",
    "        @desc: function to check if the column has alphanumeric entries\n",
    "        \"\"\"\n",
    "        return bool(re.fullmatch(r'^[a-zA-Z0-9_]*$', in_str))\n",
    "\n",
    "    @staticmethod\n",
    "    def has_yyyy_mm_dd_format(in_str):\n",
    "        \"\"\"\n",
    "        @desc: function to decide if the a column of a data has date format yyyy-mm-dd\n",
    "        \"\"\"\n",
    "        return bool(re.fullmatch(r'\\d{4}-\\d{2}-\\d{2}', in_str))\n",
    "\n",
    "    @staticmethod\n",
    "    def convert_date_to_yyyy_mm_dd(in_column : pd.core.series.Series):\n",
    "        \"\"\"\n",
    "        @desc: function to set the date column with date format yyyy-mm-dd\n",
    "        \"\"\"\n",
    "        in_column = in_column.apply(parse)\n",
    "        in_column = pd.to_datetime(in_column, infer_datetime_format=True, errors='coerce')\n",
    "        \n",
    "        return in_column\n",
    "\n",
    "    # ================================================================== #\n",
    "    #                   PRIMARY FUNCTIONS\n",
    "    # ================================================================== #\n",
    "    def clean_orders_data(self, orders_df : pd.DataFrame):\n",
    "        \"\"\"\n",
    "        @desc: pre-process the orders table\n",
    "        \"\"\"\n",
    "        order_processed_df = orders_df.copy().drop_duplicates()\n",
    "        order_processed_df = order_processed_df.drop(columns={'first_name', 'last_name', '1'})\n",
    "        \n",
    "        return order_processed_df\n",
    "\n",
    "    def clean_user_data(self, users_df : pd.DataFrame, table_name : str = 'legacy_users'):\n",
    "        \"\"\"\n",
    "        @desc: pre-process the legacy users table\n",
    "        \"\"\"\n",
    "        if users_df.isnull().sum().sum() and users_df.isna().sum().sum():\n",
    "            raise f\"The database : {table_name}, has total {users_df.isnull().sum().sum()} NULL values and {users_df.isna().sum().sum()} NaN values\"\n",
    "        else:\n",
    "            print(f\"[usrmsg] No NULLs or NaNs found in {table_name}\")\n",
    "        users_df_processed = users_df.copy()\n",
    "        users_df_processed = users_df[~users_df.apply(lambda row: row.astype(str).str.contains('NULL').any(), axis=1)]\n",
    "        # check for data types \n",
    "        #   -1) always begin with dropping duplicates and storing as a seperate file\n",
    "        users_df_processed = users_df_processed.drop_duplicates()\n",
    "        #   -2) set all columns except index to be of string format\n",
    "        str_convert_dict = {col: 'string' for col in users_df_processed.columns if col not in ['index']}\n",
    "        users_df_processed = users_df_processed.astype(str_convert_dict)\n",
    "        #   -3) remove all entries that are pure alphanumeric\n",
    "        users_df_processed = users_df_processed[~users_df_processed['email_address'].apply(is_alphanumeric)]\n",
    "        #   -4) DoB and join_date should be datetime format and of type yyyy-mm-dd\n",
    "        users_df_processed['date_of_birth'] = convert_date_to_yyyy_mm_dd(users_df_processed['date_of_birth'])\n",
    "        users_df_processed['join_date'] = convert_date_to_yyyy_mm_dd(users_df_processed['join_date'])\n",
    "        #   -5) convert all 'GGB' country code to 'GB'\n",
    "        users_df_processed['country_code'] = users_df_processed['country_code'].str.replace('GGB', 'GB', regex=False)\n",
    "\n",
    "        return users_df_processed\n",
    "    \n",
    "    def clean_card_data(self, card_df : pd.DataFrame):\n",
    "        \"\"\"\n",
    "        @desc: pre-process the card table\n",
    "        \"\"\"\n",
    "        #   -1) drop columns that are filled with missing and/or incorrect information\n",
    "        card_processed_df = card_df.copy().drop(columns=['Unnamed: 0'])\n",
    "        #  -2) always begin with dropping duplicates \n",
    "        card_processed_df = card_processed_df.drop_duplicates()\n",
    "        #   -3) remove columns with \"NULL\"\n",
    "        card_processed_df = card_processed_df[~card_processed_df.apply(lambda row: row.astype(str).str.contains('NULL').any(), axis=1)]\n",
    "        #   -4) remove all entries that are pure alphanumeric\n",
    "        card_processed_df = card_processed_df[~card_processed_df['card_provider'].apply(is_alphanumeric)]\n",
    "        #   -5) those rows that have NaN in card_number expiry_date, fill those appropriately\n",
    "        nan_card_num_expiry_date_df = card_processed_df[card_processed_df['card_number expiry_date'].isna()]\n",
    "        nan_card_num_expiry_date_df['card_number expiry_date'] = nan_card_num_expiry_date_df['card_number'].astype(str) + ' ' + nan_card_num_expiry_date_df['expiry_date'].astype(str)\n",
    "        #   -6) those rows that DONT have NaN in card_number expiry_date column, strip those isolated and replace their equivalent NaN values in the card_number and the expiry_date columns appropriately\n",
    "        not_nan_card_num_expiry_date_df = card_processed_df[~card_processed_df['card_number expiry_date'].isna()]\n",
    "        splitted_cardnumexpdate_df = not_nan_card_num_expiry_date_df['card_number expiry_date'].str.split(n=1, expand=True)\n",
    "        not_nan_card_num_expiry_date_df['card_number'], not_nan_card_num_expiry_date_df['expiry_date'] = splitted_cardnumexpdate_df[0], splitted_cardnumexpdate_df[1]\n",
    "        #   -7) combine the two to store the seperate data with no NaNs . . . (hopefully :P)\n",
    "        card_processed_df = pd.concat([nan_card_num_expiry_date_df, not_nan_card_num_expiry_date_df], ignore_index=True)\n",
    "        del nan_card_num_expiry_date_df, not_nan_card_num_expiry_date_df\n",
    "        #   -8) change all objects to string\n",
    "        card_processed_df = card_processed_df.astype('string')\n",
    "        #   -9) finally, change all date columns to datetimeformat\n",
    "        card_processed_df['date_payment_confirmed'] = convert_date_to_yyyy_mm_dd(card_processed_df['date_payment_confirmed'])\n",
    "        card_processed_df['expiry_date'] = pd.to_datetime(card_processed_df['expiry_date'], format='%m/%y') + pd.offsets.MonthEnd(0)\n",
    "\n",
    "        return card_processed_df\n",
    "    \n",
    "    def called_clean_store_data(self, store_detail_df : pd.DataFrame):\n",
    "        \"\"\"\n",
    "        @desc: pre-process the store table\n",
    "        \"\"\"\n",
    "        store_detail_processed_df = store_detail_df.copy()\n",
    "        #   -1) always begin with removing duplicates\n",
    "        store_detail_processed_df = store_detail_processed_df.drop_duplicates()\n",
    "        #   -2) remove purely nan or none columns (e.g. lat)\n",
    "        store_detail_processed_df = store_detail_processed_df.drop(columns=\"lat\")\n",
    "        store_detail_processed_df = store_detail_processed_df.drop(columns=\"address\")\n",
    "        #   -3) remove all pure alphanmueric rows\n",
    "        store_detail_processed_df = store_detail_processed_df[~store_detail_processed_df['opening_date'].apply(is_alphanumeric)]\n",
    "        #   -4) account for missing addresses, longitude and latitude values\n",
    "        # --> ANS) No need to change, it is a portal type store, and only one and unique in the table\n",
    "        #   -5) remove all alphabets in staff_numbers column\n",
    "        store_detail_processed_df[\"staff_numbers\"] = store_detail_processed_df[\"staff_numbers\"].str.replace(r'[a-zA-Z]', '', regex=True)\n",
    "        #   -6) fix format of opening_date\n",
    "        store_detail_processed_df[\"opening_date\"] = convert_date_to_yyyy_mm_dd(store_detail_processed_df[\"opening_date\"])\n",
    "        #   -7) set eeEurope and eeAmerica to Europe and America in the continent column\n",
    "        store_detail_processed_df[\"continent\"] = store_detail_processed_df[\"continent\"].str.replace('eeEurope', 'Europe')\n",
    "        store_detail_processed_df[\"continent\"] = store_detail_processed_df[\"continent\"].str.replace('eeAmerica', 'America')\n",
    "        #   -8) convert all object to string appropriately and all numbers to int and float appropriately\n",
    "        store_detail_processed_df = store_detail_processed_df.astype({col: 'string' for col in store_detail_processed_df.columns if col not in [\"index\", \"opening_date\", \"longitude\", \"staff_numbers\", \"latitude\"]})\n",
    "        # store_detail_processed_df = store_detail_processed_df.astype({col: 'float64' for col in store_detail_processed_df.columns if col in [\"longitude\", \"latitude\"]})\n",
    "        store_detail_processed_df[\"longitude\"] = pd.to_numeric(store_detail_processed_df[\"longitude\"], errors='coerce')\n",
    "        store_detail_processed_df[\"latitude\"] = pd.to_numeric(store_detail_processed_df[\"latitude\"], errors='coerce')\n",
    "        store_detail_processed_df[\"staff_numbers\"] = pd.to_numeric(store_detail_processed_df[\"staff_numbers\"], errors='coerce')\n",
    "\n",
    "        return store_detail_processed_df\n",
    "\n",
    "\n",
    "    def clean_products_data(self, products_df : pd.DataFrame):\n",
    "        \"\"\"\n",
    "        @desc: pre-process the store table\n",
    "        \"\"\"\n",
    "        products_processed_df = products_df.copy()\n",
    "        #   -1) always begin by dropping duplicates\n",
    "        products_processed_df = products_processed_df.drop_duplicates()\n",
    "        #   -2) rename the Unamed column to index\n",
    "        products_processed_df.rename(columns={'Unnamed: 0': 'index'}, inplace=True)\n",
    "        #   -3) fill nans with empty strings to allow easier processing for the rest of the cleaning\n",
    "        products_processed_df = products_processed_df.fillna('')\n",
    "        #   -4) remove all the pure alphanumeric entries\n",
    "        products_processed_df = products_processed_df[~products_processed_df['date_added'].apply(is_alphanumeric)]\n",
    "        #   -5) for weights : a) compute all multiplication expressions and replace with resultant value\n",
    "        products_processed_df[\"weight\"] = products_processed_df[\"weight\"].apply(mullexp_to_netresult)\n",
    "        #   -6) for weights : b) standardise them to 'kg'\n",
    "        products_processed_df[\"weight\"] = products_processed_df[\"weight\"].apply(convert_weights)\n",
    "        #   -7) drop £ and kg to enable weight and product price columns to be numeric\n",
    "        products_processed_df[\"product_price\"] = products_processed_df[\"product_price\"].str.replace('£', '')\n",
    "        products_processed_df.rename(columns={'product_price': 'product_price (£)'}, inplace=True)\n",
    "        products_processed_df[\"weight\"] = products_processed_df[\"weight\"].str.replace('kg', '')\n",
    "        products_processed_df.rename(columns={'weight': 'weight (kg)'}, inplace=True)\n",
    "        #   -8) set product_price, weight, EAN to be numeric\n",
    "        products_processed_df[\"product_price (£)\"] = pd.to_numeric(products_processed_df[\"product_price (£)\"], errors='coerce')\n",
    "        products_processed_df[\"weight (kg)\"] = pd.to_numeric(products_processed_df[\"weight (kg)\"], errors='coerce')\n",
    "        products_processed_df[\"EAN\"] = pd.to_numeric(products_processed_df[\"EAN\"], errors='coerce')\n",
    "        #   -9) set product_name, cateogry, uuid, removed and product_code to string  AND  date_added to datetime\n",
    "        products_processed_df = products_processed_df.astype({\"product_name\" : \"string\", \"category\" : \"string\", \"uuid\" : \"string\", \"removed\" : \"string\", \"product_code\" : \"string\"})\n",
    "        products_processed_df['date_added'] = convert_date_to_yyyy_mm_dd(products_processed_df['date_added'])\n",
    "\n",
    "        return products_processed_df\n",
    "    \n",
    "\n",
    "    def clean_event_date_data(self, events_df : pd.DataFrame):\n",
    "        \"\"\"\n",
    "        @desc: pre-process the date details table\n",
    "        \"\"\"\n",
    "        #   -1) drop duplicates and store a copy of the original\n",
    "        events_df_processed = events_df.copy().drop_duplicates()\n",
    "        #   -2) remove all entries that are purely alphanumeric in nature\n",
    "        events_df_processed = events_df_processed[~events_df_processed[\"date_uuid\"].apply(is_alphanumeric)]\n",
    "        #   -3) use info from year, month, day and timestamp to set a seperate datetime column\n",
    "        events_df_processed['datetime'] = pd.to_datetime(events_df_processed[['year', 'month', 'day', 'timestamp']].astype(str).agg(' '.join, axis=1), format='%Y %m %d %H:%M:%S')\n",
    "        #   -4) set timestamp, timeperiod and date_uuid as string\n",
    "        events_df_processed = events_df_processed.astype({\"timestamp\" : \"string\", \"time_period\" : \"string\", \"date_uuid\" : \"string\"})\n",
    "        #   -5) set month, year and day as int32\n",
    "        events_df_processed[\"month\"] = pd.to_numeric(events_df_processed[\"month\"], errors='coerce')\n",
    "        events_df_processed[\"year\"] = pd.to_numeric(events_df_processed[\"year\"], errors='coerce')\n",
    "        events_df_processed[\"day\"] = pd.to_numeric(events_df_processed[\"day\"], errors='coerce')\n",
    "\n",
    "        return events_df_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ingest the datasets\n",
    "connector = DatabaseConnector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11800\\149431661.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mextractor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataExtractor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconnector\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: __init__() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "extractor = DataExtractor(connector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_alpha(in_str):\n",
    "    \"\"\"\n",
    "    @desc: function to check if the column has alphanumeric entries\n",
    "    \"\"\"\n",
    "    return any(c.isalpha() for c in in_str)\n",
    "\n",
    "def is_alphanumeric(in_str):\n",
    "    \"\"\"\n",
    "    @desc: function to check if the column has alphanumeric entries\n",
    "    \"\"\"\n",
    "    return bool(re.fullmatch(r'^[a-zA-Z0-9_]*$', in_str))\n",
    "\n",
    "def has_yyyy_mm_dd_format(in_str):\n",
    "    \"\"\"\n",
    "    @desc: function to decide if the a column of a data has date format yyyy-mm-dd\n",
    "    \"\"\"\n",
    "    return bool(re.fullmatch(r'\\d{4}-\\d{2}-\\d{2}', in_str))\n",
    "\n",
    "def convert_date_to_yyyy_mm_dd(in_column : pd.core.series.Series):\n",
    "    \"\"\"\n",
    "    @desc: function to set the date column with date format yyyy-mm-dd\n",
    "    \"\"\"\n",
    "    in_column = in_column.apply(parse)\n",
    "    in_column = pd.to_datetime(in_column, infer_datetime_format=True, errors='coerce')\n",
    "    \n",
    "    return in_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for nulls or NaNs, alternative is np.unique(users_df.isnull()), or just use df.info()\n",
    "if users_df.isnull().sum().sum() and users_df.isna().sum().sum():\n",
    "    raise f\"The database : {table_name}, has total {users_df.isnull().sum().sum()} NULL values and {users_df.isna().sum().sum()} NaN values\"\n",
    "else:\n",
    "    print(f\"[usrmsg] No NULLs or NaNs found in {table_name}\")\n",
    "\n",
    "users_df_processed = users_df[~users_df.apply(lambda row: row.astype(str).str.contains('NULL').any(), axis=1)]\n",
    "\n",
    "# check for data types \n",
    "#   -1) always begin with dropping duplicates and storing as a seperate file\n",
    "users_df_processed = users_df_processed.drop_duplicates()\n",
    "#   -2) set all columns except index to be of string format\n",
    "str_convert_dict = {col: 'string' for col in users_df_processed.columns if col not in ['index']}\n",
    "users_df_processed = users_df_processed.astype(str_convert_dict)\n",
    "#   -3) remove all entries that are pure alphanumeric\n",
    "users_df_processed = users_df_processed[~users_df_processed['email_address'].apply(is_alphanumeric)]\n",
    "#   -4) DoB and join_date should be datetime format and of type yyyy-mm-dd\n",
    "users_df_processed['date_of_birth'] = convert_date_to_yyyy_mm_dd(users_df_processed['date_of_birth'])\n",
    "users_df_processed['join_date'] = convert_date_to_yyyy_mm_dd(users_df_processed['join_date'])\n",
    "#   -5) convert all 'GGB' country code to 'GB'\n",
    "users_df_processed['country_code'] = users_df_processed['country_code'].str.replace('GGB', 'GB', regex=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update your GitHub repository with the latest code changes from your local project. Start by staging your modifications and creating a commit. Then, push the changes to your GitHub repository.\n",
    "\n",
    "Additionally, document your progress by adding to your GitHub README file. You can refer to the relevant lesson in the prerequisites for this task for more information.\n",
    "\n",
    "At minimum, your README file should contain the following information:\n",
    "\n",
    "Project Title\n",
    "\n",
    "> Table of Contents, if the README file is long\n",
    "\n",
    "> A description of the project: what it does, the aim of the project, and what you learned\n",
    "\n",
    "> Installation instructions\n",
    "\n",
    "> Usage instructions\n",
    "\n",
    "> File structure of the project\n",
    "\n",
    "License information\n",
    "* You don't have to write all of this at once, but make sure to update your README file as you go along, so that you don't forget to add anything.\n",
    "\n",
    "* Refactoring will be a continuous and constant process, but this is the time to really scrutinise your code.\n",
    "\n",
    "You can use the following list to make improvements:\n",
    "\n",
    "* Meaningful Naming: Use descriptive names for methods and variables to enhance code readability. For example, create_list_of_website_links() over links() and use for element in web_element_list instead of for i in list.\n",
    "* Eliminate Code Duplication: Identify repeated code blocks and refactor them into separate methods or functions. This promotes code reusability and reduces the likelihood of bugs.\n",
    "* Single Responsibility Principle (SRP): Ensure that each method has a single responsibility, focusing on a specific task. If a method handles multiple concerns, split it into smaller, focused methods.\n",
    "* Access Modifiers: Make methods private or protected if they are intended for internal use within the class and not externally accessible\n",
    "* Main Script Execution: Use the if __name__ == \"__main__\": statement to include code blocks that should only run when the script is executed directly, not when imported as a module\n",
    "* Consistent Import Order: Organize import statements in a consistent manner, such as alphabetically, and place from statements before import statements to maintain readability\n",
    "* Avoid Nested Loops: Minimize nested loops whenever possible to improve code efficiency and reduce complexity\n",
    "* Minimal Use of self: When writing methods in a class, only use self for variables that store information unique to each object created from the class. This helps keep the code organized and ensures that each object keeps its own special data separate from others.\n",
    "* Avoid import *: Import only the specific methods or classes needed from a module to enhance code clarity and prevent naming conflicts\n",
    "* Consistent Docstrings: Provide clear and consistent docstrings for all methods, explaining their purpose, parameters, and return values. This aids code understanding for other developers.\n",
    "* Type Annotations: Consider adding type annotations to method signatures, variables, and return values to improve code maintainability and catch type-related errors during development"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "airdmp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
