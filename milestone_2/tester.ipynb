{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install SQLAlchemy\n",
    "# !pip install psycopg2\n",
    "# !pip install pandas\n",
    "# !pip install PyYAML\n",
    "# !pip install tabula-py\n",
    "# !pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine, MetaData, Table, select\n",
    "from dateutil.parser import parse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import psycopg2\n",
    "import yaml\n",
    "import re\n",
    "import tabula\n",
    "\n",
    "# using the requests library to GET the number of stores\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3 -- Users Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test code for Psycopg2\n",
    "# conn = psycopg2.connect(\n",
    "#     host=ENDPOINT,\n",
    "#     port=PORT,\n",
    "#     database=DATABASE,\n",
    "#     user=USER,\n",
    "#     password=PASSWORD\n",
    "# )\n",
    "yaml_file_path = '../db_creds.yaml'\n",
    "\n",
    "# Read the YAML file and store its contents in a Python data structure (dictionary)\n",
    "with open(yaml_file_path, 'r') as file:\n",
    "    yaml_data = yaml.safe_load(file)\n",
    "\n",
    "# credentials\n",
    "DATABASE_TYPE = 'postgresql'\n",
    "# DBAPI = 'psycopg2'\n",
    "ENDPOINT = yaml_data['RDS_HOST']\n",
    "USER = yaml_data['RDS_USER']\n",
    "PASSWORD = yaml_data['RDS_PASSWORD']\n",
    "PORT = yaml_data['RDS_PORT']\n",
    "DATABASE = yaml_data['RDS_DATABASE']\n",
    "\n",
    "# setup sql engine and connect\n",
    "sql_engine = create_engine(f\"{DATABASE_TYPE}://{USER}:{PASSWORD}@{ENDPOINT}:{PORT}/{DATABASE}\")\n",
    "sql_connection = sql_engine.connect()\n",
    "\n",
    "# metadata, holds collection of table info, their data types, schema names etc., obtained from here : https://docs.sqlalchemy.org/en/20/core/metadata.html\n",
    "# pros of MetaData() includes thread safety -> meaning it can handle concurrent tasks from multiple thread (computationally efficient when multiple threads need access to same resource)\n",
    "metadata = MetaData()\n",
    "metadata.reflect(sql_engine)\n",
    "table_names = metadata.tables.keys()\n",
    "\n",
    "# reflect allows us\n",
    "names_ = list(table_names)\n",
    "\n",
    "# read table\n",
    "users_table = sql_connection.execute(select(Table('legacy_users', metadata, autoload=True, autoload_with=sql_engine)))\n",
    "headers = users_table.keys()\n",
    "users_df = pd.DataFrame(users_table.fetchall(), columns=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = 'legacy_users'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_alphanumeric(in_str):\n",
    "    \"\"\"\n",
    "    @desc: function to check if the column has alphanumeric entries\n",
    "    \"\"\"\n",
    "    return bool(re.fullmatch(r'^[a-zA-Z0-9_]*$', in_str))\n",
    "\n",
    "def has_yyyy_mm_dd_format(in_str):\n",
    "    \"\"\"\n",
    "    @desc: function to decide if the a column of a data has date format yyyy-mm-dd\n",
    "    \"\"\"\n",
    "    return bool(re.fullmatch(r'\\d{4}-\\d{2}-\\d{2}', in_str))\n",
    "\n",
    "def convert_date_to_yyyy_mm_dd(in_column : pd.core.series.Series):\n",
    "    \"\"\"\n",
    "    @desc: function to set the date column with date format yyyy-mm-dd\n",
    "    \"\"\"\n",
    "    in_column = in_column.apply(parse)\n",
    "    in_column = pd.to_datetime(in_column, infer_datetime_format=True, errors='coerce')\n",
    "    \n",
    "    return in_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for nulls or NaNs, alternative is np.unique(users_df.isnull()), or just use df.info()\n",
    "if users_df.isnull().sum().sum() and users_df.isna().sum().sum():\n",
    "    raise f\"The database : {table_name}, has total {users_df.isnull().sum().sum()} NULL values and {users_df.isna().sum().sum()} NaN values\"\n",
    "else:\n",
    "    print(f\"[usrmsg] No NULLs or NaNs found in {table_name}\")\n",
    "\n",
    "users_df_processed = users_df[~users_df.apply(lambda row: row.astype(str).str.contains('NULL').any(), axis=1)]\n",
    "\n",
    "# check for data types \n",
    "#   -1) always begin with dropping duplicates and storing as a seperate file\n",
    "users_df_processed = users_df_processed.drop_duplicates()\n",
    "#   -2) set all columns except index to be of string format\n",
    "str_convert_dict = {col: 'string' for col in users_df_processed.columns if col not in ['index']}\n",
    "users_df_processed = users_df_processed.astype(str_convert_dict)\n",
    "#   -3) remove all entries that are pure alphanumeric\n",
    "users_df_processed = users_df_processed[~users_df_processed['email_address'].apply(is_alphanumeric)]\n",
    "#   -4) DoB and join_date should be datetime format and of type yyyy-mm-dd\n",
    "users_df_processed['date_of_birth'] = convert_date_to_yyyy_mm_dd(users_df_processed['date_of_birth'])\n",
    "users_df_processed['join_date'] = convert_date_to_yyyy_mm_dd(users_df_processed['join_date'])\n",
    "#   -5) convert all 'GGB' country code to 'GB'\n",
    "users_df_processed['country_code'] = users_df_processed['country_code'].str.replace('GGB', 'GB', regex=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_df_processed.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_file_path = '../local_db_creds.yaml'\n",
    "\n",
    "# Read the YAML file and store its contents in a Python data structure (dictionary)\n",
    "with open(yaml_file_path, 'r') as file:\n",
    "    yaml_data = yaml.safe_load(file)\n",
    "\n",
    "# credentials\n",
    "DATABASE_TYPE = 'postgresql+psycopg2'\n",
    "ENDPOINT = yaml_data['LOCAL_HOST']\n",
    "USER = yaml_data['LOCAL_USER']\n",
    "PASSWORD = yaml_data['LOCAL_PASSWORD']\n",
    "PORT = yaml_data['LOCAL_PORT']\n",
    "DATABASE = yaml_data['LOCAL_DATABASE']\n",
    "\n",
    "engine = create_engine(f\"{DATABASE_TYPE}://{USER}:{PASSWORD}@{ENDPOINT}:{PORT}/{DATABASE}\")\n",
    "connection = engine.connect()\n",
    "users_df_processed.to_sql(\"dim_users\", engine, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4 -- Card Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the .pdf file\n",
    "pdf_path = \"../card_details.pdf\"\n",
    "card_df_list = tabula.read_pdf(pdf_path, stream=True, pages='all')\n",
    "card_df = pd.concat(card_df_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "card_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   -1) drop columns that are filled with missing and/or incorrect information\n",
    "card_processed_df = card_df.drop(columns=['Unnamed: 0'])\n",
    " #  -2) always begin with dropping duplicates \n",
    "card_processed_df = card_processed_df.drop_duplicates()\n",
    "#   -3) remove columns with \"NULL\"\n",
    "card_processed_df = card_processed_df[~card_processed_df.apply(lambda row: row.astype(str).str.contains('NULL').any(), axis=1)]\n",
    "#   -4) remove all entries that are pure alphanumeric\n",
    "card_processed_df = card_processed_df[~card_processed_df['card_provider'].apply(is_alphanumeric)]\n",
    "#   -5) those rows that have NaN in card_number expiry_date, fill those appropriately\n",
    "nan_card_num_expiry_date_df = card_processed_df[card_processed_df['card_number expiry_date'].isna()]\n",
    "nan_card_num_expiry_date_df['card_number expiry_date'] = nan_card_num_expiry_date_df['card_number'].astype(str) + ' ' + nan_card_num_expiry_date_df['expiry_date'].astype(str)\n",
    "#   -6) those rows that DONT have NaN in card_number expiry_date column, strip those isolated and replace their equivalent NaN values in the card_number and the expiry_date columns appropriately\n",
    "not_nan_card_num_expiry_date_df = card_processed_df[~card_processed_df['card_number expiry_date'].isna()]\n",
    "splitted_cardnumexpdate_df = not_nan_card_num_expiry_date_df['card_number expiry_date'].str.split(n=1, expand=True)\n",
    "not_nan_card_num_expiry_date_df['card_number'], not_nan_card_num_expiry_date_df['expiry_date'] = splitted_cardnumexpdate_df[0], splitted_cardnumexpdate_df[1]\n",
    "#   -7) combine the two to store the seperate data with no NaNs . . . (hopefully :P)\n",
    "card_processed_df = pd.concat([nan_card_num_expiry_date_df, not_nan_card_num_expiry_date_df], ignore_index=True)\n",
    "del nan_card_num_expiry_date_df, not_nan_card_num_expiry_date_df\n",
    "#   -8) change all objects to string\n",
    "card_processed_df = card_processed_df.astype('string')\n",
    "#   -9) finally, change all date columns to datetimeformat\n",
    "card_processed_df['date_payment_confirmed'] = convert_date_to_yyyy_mm_dd(card_processed_df['date_payment_confirmed'])\n",
    "card_processed_df['expiry_date'] = pd.to_datetime(card_processed_df['expiry_date'], format='%m/%y') + pd.offsets.MonthEnd(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_file_path = '../local_db_creds.yaml'\n",
    "\n",
    "# Read the YAML file and store its contents in a Python data structure (dictionary)\n",
    "with open(yaml_file_path, 'r') as file:\n",
    "    yaml_data = yaml.safe_load(file)\n",
    "\n",
    "# credentials\n",
    "DATABASE_TYPE = 'postgresql+psycopg2'\n",
    "ENDPOINT = yaml_data['LOCAL_HOST']\n",
    "USER = yaml_data['LOCAL_USER']\n",
    "PASSWORD = yaml_data['LOCAL_PASSWORD']\n",
    "PORT = yaml_data['LOCAL_PORT']\n",
    "DATABASE = yaml_data['LOCAL_DATABASE']\n",
    "\n",
    "engine = create_engine(f\"{DATABASE_TYPE}://{USER}:{PASSWORD}@{ENDPOINT}:{PORT}/{DATABASE}\")\n",
    "connection = engine.connect()\n",
    "card_processed_df.to_sql(\"dim_card_details\", engine, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 5 -- Store Related Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_alphanumeric(in_str):\n",
    "    \"\"\"\n",
    "    @desc: function to check if the column has alphanumeric entries\n",
    "    \"\"\"\n",
    "    return bool(re.fullmatch(r'^[a-zA-Z0-9_]*$', in_str))\n",
    "\n",
    "def has_yyyy_mm_dd_format(in_str):\n",
    "    \"\"\"\n",
    "    @desc: function to decide if the a column of a data has date format yyyy-mm-dd\n",
    "    \"\"\"\n",
    "    return bool(re.fullmatch(r'\\d{4}-\\d{2}-\\d{2}', in_str))\n",
    "\n",
    "def convert_date_to_yyyy_mm_dd(in_column : pd.core.series.Series):\n",
    "    \"\"\"\n",
    "    @desc: function to set the date column with date format yyyy-mm-dd\n",
    "    \"\"\"\n",
    "    in_column = in_column.apply(parse)\n",
    "    in_column = pd.to_datetime(in_column, infer_datetime_format=True, errors='coerce')\n",
    "    \n",
    "    return in_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_file_path = '../api_key.yaml'\n",
    "\n",
    "# Read the YAML file and store its contents in a Python data structure (dictionary)\n",
    "with open(yaml_file_path, 'r') as file:\n",
    "    yaml_data = yaml.safe_load(file)\n",
    "\n",
    "\n",
    "# Retrieve a store: https://aqj7u5id95.execute-api.eu-west-1.amazonaws.com/prod/store_details/{store_number}\n",
    "# Return the number of stores: https://aqj7u5id95.execute-api.eu-west-1.amazonaws.com/prod/number_stores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Common methods include `GET` (retrieve resource), `POST` (create resource), `PUT` (update resource), and `DELETE` (remove resource). We will discuss in detail about them later\n",
    "\n",
    "- **Status Codes**: HTTP status codes indicate the result of the request. These codes range from informational (`1xx`) to success (`2xx`), redirection (`3xx`), client errors (`4xx`), and server errors (`5xx`).   \n",
    "\n",
    "- **Port**: HTTPS typically uses port 443 for communication, while HTTP uses port 80. The use of a different port helps differentiate between secure and non-secure connections.\n",
    "\n",
    "- The request contains information such as the `HTTP method`, `the endpoint URL`, optional `headers`, and, in some cases, a `payload` or `body`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **HTTP Method**: The HTTP method, also known as the HTTP verb, specifies the action to be performed on the resource. Common HTTP methods include `GET` (retrieve data), `POST` (create data), `PUT` (update data), and `DELETE` (remove data).\n",
    "\n",
    "- **Endpoint URL**: The **endpoint URL** represents the specific resource or functionality on the server that the client wants to interact with. It typically follows a specific URL pattern defined by the API.\n",
    "\n",
    "- **Headers**: Headers provide additional information about the request, such as content type, authorization tokens, or caching directives. We will learn more about headers  in a later lesson.\n",
    "\n",
    "- **Payload or Body**: In certain cases, requests may include a payload or body `containing data to be sent to the server`. This is common for methods like `POST` or `PUT`, where the payload contains the data to create or update a resource."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Endpoint URLs have the usual format of  . . .\n",
    "```\n",
    "<ROOT_URL>/<Path>?<Query Parameters>\n",
    "```\n",
    "\n",
    "In this structure:\n",
    "- `<ROOT_URL>` represents the base URL of the API\n",
    "- `<Path>` refers to the specific path or endpoint within the API that offers a specific service\n",
    "- `<Query Parameters>` are optional parameters passed in the URL query string, allowing for additional customization or filtering of the request\n",
    "\n",
    "After the path section of the endpoint URL, one or more *parameters* can be specified\n",
    "\n",
    "* The `?` symbol denotes the separation between the endpoint path and the start of the parameters\n",
    "* while multiple parameters are typically separated using the `&` symbol.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get('https://aqj7u5id95.execute-api.eu-west-1.amazonaws.com/prod/number_stores')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'yFBQbwXe9J3sd6zWVAMrK6lcxxr0q1lr2PT6DDMX'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yaml_data[\"API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_number = 450\n",
    "store_detail = []\n",
    "\n",
    "get_all_stores_url = \"https://aqj7u5id95.execute-api.eu-west-1.amazonaws.com/prod/number_stores\"\n",
    "headers = {\n",
    "    \"X-API-KEY\": yaml_data[\"API_KEY\"]\n",
    "}\n",
    "\n",
    "for i in range(store_number):\n",
    "    get_store_number_url = f\"https://aqj7u5id95.execute-api.eu-west-1.amazonaws.com/prod/store_details/{i}\"\n",
    "    response = requests.get(get_store_number_url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        # Access the response data as JSON\n",
    "        store_detail.append(response.json())\n",
    "\n",
    "    else:\n",
    "        print(f\"Request failed with status code: {response.status_code}\")\n",
    "        print(f\"Response Text: {response.text}\")\n",
    "assert len(store_detail) == 450"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(store_detail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: pikachu\n",
      "Abilities: static, lightning-rod\n",
      "Base Experience: 112\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Send a GET request to the Pokémon API to retrieve information about Pikachu\n",
    "response = requests.get('https://pokeapi.co/api/v2/pokemon/pikachu')\n",
    "\n",
    "# Check if the request was successful (status code 200)\n",
    "if response.status_code == 200:\n",
    "    # Access the response data as JSON\n",
    "    data = response.json()\n",
    "\n",
    "    # Extract and print the name of the Pokémon\n",
    "    name = data['name']\n",
    "    print(f\"Name: {name}\")\n",
    "\n",
    "    # Extract and print the Pokémon's abilities\n",
    "    abilities = [ability['ability']['name'] for ability in data['abilities']]\n",
    "    print(\"Abilities:\", \", \".join(abilities))\n",
    "\n",
    "    # Extract and print the Pokémon's base experience\n",
    "    base_experience = data['base_experience']\n",
    "    print(f\"Base Experience: {base_experience}\")\n",
    "\n",
    "# If the request was not successful, print the status code and response text\n",
    "else:\n",
    "    print(f\"Request failed with status code: {response.status_code}\")\n",
    "    print(f\"Response Text: {response.text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "airdmp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
