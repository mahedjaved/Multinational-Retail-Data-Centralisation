{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install SQLAlchemy\n",
    "# !pip install psycopg2\n",
    "# !pip install pandas\n",
    "# !pip install PyYAML\n",
    "# !pip install tabula-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine, MetaData, Table, select\n",
    "from dateutil.parser import parse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import psycopg2\n",
    "import yaml\n",
    "import re\n",
    "import tabula"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3 -- Users Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test code for Psycopg2\n",
    "# conn = psycopg2.connect(\n",
    "#     host=ENDPOINT,\n",
    "#     port=PORT,\n",
    "#     database=DATABASE,\n",
    "#     user=USER,\n",
    "#     password=PASSWORD\n",
    "# )\n",
    "yaml_file_path = '../db_creds.yaml'\n",
    "\n",
    "# Read the YAML file and store its contents in a Python data structure (dictionary)\n",
    "with open(yaml_file_path, 'r') as file:\n",
    "    yaml_data = yaml.safe_load(file)\n",
    "\n",
    "# credentials\n",
    "DATABASE_TYPE = 'postgresql'\n",
    "# DBAPI = 'psycopg2'\n",
    "ENDPOINT = yaml_data['RDS_HOST']\n",
    "USER = yaml_data['RDS_USER']\n",
    "PASSWORD = yaml_data['RDS_PASSWORD']\n",
    "PORT = yaml_data['RDS_PORT']\n",
    "DATABASE = yaml_data['RDS_DATABASE']\n",
    "\n",
    "# setup sql engine and connect\n",
    "sql_engine = create_engine(f\"{DATABASE_TYPE}://{USER}:{PASSWORD}@{ENDPOINT}:{PORT}/{DATABASE}\")\n",
    "sql_connection = sql_engine.connect()\n",
    "\n",
    "# metadata, holds collection of table info, their data types, schema names etc., obtained from here : https://docs.sqlalchemy.org/en/20/core/metadata.html\n",
    "# pros of MetaData() includes thread safety -> meaning it can handle concurrent tasks from multiple thread (computationally efficient when multiple threads need access to same resource)\n",
    "metadata = MetaData()\n",
    "metadata.reflect(sql_engine)\n",
    "table_names = metadata.tables.keys()\n",
    "\n",
    "# reflect allows us\n",
    "names_ = list(table_names)\n",
    "\n",
    "# read table\n",
    "users_table = sql_connection.execute(select(Table('legacy_users', metadata, autoload=True, autoload_with=sql_engine)))\n",
    "headers = users_table.keys()\n",
    "users_df = pd.DataFrame(users_table.fetchall(), columns=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = 'legacy_users'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_alphanumeric(in_str):\n",
    "    \"\"\"\n",
    "    @desc: function to check if the column has alphanumeric entries\n",
    "    \"\"\"\n",
    "    return bool(re.fullmatch(r'^[a-zA-Z0-9_]*$', in_str))\n",
    "\n",
    "def has_yyyy_mm_dd_format(in_str):\n",
    "    \"\"\"\n",
    "    @desc: function to decide if the a column of a data has date format yyyy-mm-dd\n",
    "    \"\"\"\n",
    "    return bool(re.fullmatch(r'\\d{4}-\\d{2}-\\d{2}', in_str))\n",
    "\n",
    "def convert_date_to_yyyy_mm_dd(in_column : pd.core.series.Series):\n",
    "    \"\"\"\n",
    "    @desc: function to set the date column with date format yyyy-mm-dd\n",
    "    \"\"\"\n",
    "    in_column = in_column.apply(parse)\n",
    "    in_column = pd.to_datetime(in_column, infer_datetime_format=True, errors='coerce')\n",
    "    \n",
    "    return in_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for nulls or NaNs, alternative is np.unique(users_df.isnull()), or just use df.info()\n",
    "if users_df.isnull().sum().sum() and users_df.isna().sum().sum():\n",
    "    raise f\"The database : {table_name}, has total {users_df.isnull().sum().sum()} NULL values and {users_df.isna().sum().sum()} NaN values\"\n",
    "else:\n",
    "    print(f\"[usrmsg] No NULLs or NaNs found in {table_name}\")\n",
    "\n",
    "users_df_processed = users_df[~users_df.apply(lambda row: row.astype(str).str.contains('NULL').any(), axis=1)]\n",
    "\n",
    "# check for data types \n",
    "#   -1) always begin with dropping duplicates and storing as a seperate file\n",
    "users_df_processed = users_df_processed.drop_duplicates()\n",
    "#   -2) set all columns except index to be of string format\n",
    "str_convert_dict = {col: 'string' for col in users_df_processed.columns if col not in ['index']}\n",
    "users_df_processed = users_df_processed.astype(str_convert_dict)\n",
    "#   -3) remove all entries that are pure alphanumeric\n",
    "users_df_processed = users_df_processed[~users_df_processed['email_address'].apply(is_alphanumeric)]\n",
    "#   -4) DoB and join_date should be datetime format and of type yyyy-mm-dd\n",
    "users_df_processed['date_of_birth'] = convert_date_to_yyyy_mm_dd(users_df_processed['date_of_birth'])\n",
    "users_df_processed['join_date'] = convert_date_to_yyyy_mm_dd(users_df_processed['join_date'])\n",
    "#   -5) convert all 'GGB' country code to 'GB'\n",
    "users_df_processed['country_code'] = users_df_processed['country_code'].str.replace('GGB', 'GB', regex=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_df_processed.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_file_path = '../local_db_creds.yaml'\n",
    "\n",
    "# Read the YAML file and store its contents in a Python data structure (dictionary)\n",
    "with open(yaml_file_path, 'r') as file:\n",
    "    yaml_data = yaml.safe_load(file)\n",
    "\n",
    "# credentials\n",
    "DATABASE_TYPE = 'postgresql+psycopg2'\n",
    "ENDPOINT = yaml_data['LOCAL_HOST']\n",
    "USER = yaml_data['LOCAL_USER']\n",
    "PASSWORD = yaml_data['LOCAL_PASSWORD']\n",
    "PORT = yaml_data['LOCAL_PORT']\n",
    "DATABASE = yaml_data['LOCAL_DATABASE']\n",
    "\n",
    "engine = create_engine(f\"{DATABASE_TYPE}://{USER}:{PASSWORD}@{ENDPOINT}:{PORT}/{DATABASE}\")\n",
    "connection = engine.connect()\n",
    "users_df_processed.to_sql(\"dim_users\", engine, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4 -- Card Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Java HotSpot(TM) 64-Bit Server VM warning: CodeCache is full. Compiler has been disabled.\n",
      "Java HotSpot(TM) 64-Bit Server VM warning: Try increasing the code cache size using -XX:ReservedCodeCacheSize=\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CodeCache: size=131072Kb used=6603Kb max_used=6603Kb free=124468Kb\n",
      " bounds [0x0000000171580000, 0x0000000171c00000, 0x0000000179580000]\n",
      " total_blobs=2276 nmethods=1794 adapters=397\n",
      " compilation: disabled (not enough contiguous free space left)\n"
     ]
    }
   ],
   "source": [
    "# read the .pdf file\n",
    "pdf_path = \"../card_details.pdf\"\n",
    "card_df_list = tabula.read_pdf(pdf_path, stream=True, pages='all')\n",
    "card_df = pd.concat(card_df_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['card_number', 'expiry_date', 'card_provider', 'date_payment_confirmed',\n",
       "       'card_number expiry_date', 'Unnamed: 0'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "card_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   -1) drop columns that are filled with missing and/or incorrect information\n",
    "card_processed_df = card_df.drop(columns=['Unnamed: 0'])\n",
    " #  -2) always begin with dropping duplicates \n",
    "card_processed_df = card_processed_df.drop_duplicates()\n",
    "#   -3) remove columns with \"NULL\"\n",
    "card_processed_df = card_processed_df[~card_processed_df.apply(lambda row: row.astype(str).str.contains('NULL').any(), axis=1)]\n",
    "#   -4) remove all entries that are pure alphanumeric\n",
    "card_processed_df = card_processed_df[~card_processed_df['card_provider'].apply(is_alphanumeric)]\n",
    "#   -5) those rows that have NaN in card_number expiry_date, fill those appropriately\n",
    "nan_card_num_expiry_date_df = card_processed_df[card_processed_df['card_number expiry_date'].isna()]\n",
    "nan_card_num_expiry_date_df['card_number expiry_date'] = nan_card_num_expiry_date_df['card_number'].astype(str) + ' ' + nan_card_num_expiry_date_df['expiry_date'].astype(str)\n",
    "#   -6) those rows that DONT have NaN in card_number expiry_date column, strip those isolated and replace their equivalent NaN values in the card_number and the expiry_date columns appropriately\n",
    "not_nan_card_num_expiry_date_df = card_processed_df[~card_processed_df['card_number expiry_date'].isna()]\n",
    "splitted_cardnumexpdate_df = not_nan_card_num_expiry_date_df['card_number expiry_date'].str.split(n=1, expand=True)\n",
    "not_nan_card_num_expiry_date_df['card_number'], not_nan_card_num_expiry_date_df['expiry_date'] = splitted_cardnumexpdate_df[0], splitted_cardnumexpdate_df[1]\n",
    "#   -7) combine the two to store the seperate data with no NaNs . . . (hopefully :P)\n",
    "card_processed_df = pd.concat([nan_card_num_expiry_date_df, not_nan_card_num_expiry_date_df], ignore_index=True)\n",
    "del nan_card_num_expiry_date_df, not_nan_card_num_expiry_date_df\n",
    "#   -8) change all objects to string\n",
    "card_processed_df = card_processed_df.astype('string')\n",
    "#   -9) finally, change all date columns to datetimeformat\n",
    "card_processed_df['date_payment_confirmed'] = convert_date_to_yyyy_mm_dd(card_processed_df['date_payment_confirmed'])\n",
    "card_processed_df['expiry_date'] = pd.to_datetime(card_processed_df['expiry_date'], format='%m/%y') + pd.offsets.MonthEnd(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_file_path = '../local_db_creds.yaml'\n",
    "\n",
    "# Read the YAML file and store its contents in a Python data structure (dictionary)\n",
    "with open(yaml_file_path, 'r') as file:\n",
    "    yaml_data = yaml.safe_load(file)\n",
    "\n",
    "# credentials\n",
    "DATABASE_TYPE = 'postgresql+psycopg2'\n",
    "ENDPOINT = yaml_data['LOCAL_HOST']\n",
    "USER = yaml_data['LOCAL_USER']\n",
    "PASSWORD = yaml_data['LOCAL_PASSWORD']\n",
    "PORT = yaml_data['LOCAL_PORT']\n",
    "DATABASE = yaml_data['LOCAL_DATABASE']\n",
    "\n",
    "engine = create_engine(f\"{DATABASE_TYPE}://{USER}:{PASSWORD}@{ENDPOINT}:{PORT}/{DATABASE}\")\n",
    "connection = engine.connect()\n",
    "users_df_processed.to_sql(\"dim_users\", engine, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for nulls or NaNs, alternative is np.unique(users_df.isnull()), or just use df.info()\n",
    "if users_df.isnull().sum().sum() and users_df.isna().sum().sum():\n",
    "    raise f\"The database : {table_name}, has total {users_df.isnull().sum().sum()} NULL values and {users_df.isna().sum().sum()} NaN values\"\n",
    "else:\n",
    "    print(f\"[usrmsg] No NULLs or NaNs found in {table_name}\")\n",
    "\n",
    "users_df_processed = users_df[~users_df.apply(lambda row: row.astype(str).str.contains('NULL').any(), axis=1)]\n",
    "\n",
    "# check for data types \n",
    "#   -1) always begin with dropping duplicates and storing as a seperate file\n",
    "users_df_processed = users_df_processed.drop_duplicates()\n",
    "#   -2) set all columns except index to be of string format\n",
    "str_convert_dict = {col: 'string' for col in users_df_processed.columns if col not in ['index']}\n",
    "users_df_processed = users_df_processed.astype(str_convert_dict)\n",
    "#   -3) remove all entries that are pure alphanumeric\n",
    "users_df_processed = users_df_processed[~users_df_processed['email_address'].apply(is_alphanumeric)]\n",
    "#   -4) DoB and join_date should be datetime format and of type yyyy-mm-dd\n",
    "users_df_processed['date_of_birth'] = convert_date_to_yyyy_mm_dd(users_df_processed['date_of_birth'])\n",
    "users_df_processed['join_date'] = convert_date_to_yyyy_mm_dd(users_df_processed['join_date'])\n",
    "#   -5) convert all 'GGB' country code to 'GB'\n",
    "users_df_processed['country_code'] = users_df_processed['country_code'].str.replace('GGB', 'GB', regex=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "removed_expiry_na_df = card_df[~card_df[\"expiry_date\"].isna()]\n",
    "# removed_expiry_na_df = removed_expiry_na_df[removed_expiry_na_df[\"card_number expiry_date\"].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(users_df_processed.iloc[1996], users_df_processed.iloc[1046], users_df_processed.iloc[866])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the date of the column has the format yyyy-mm-dd\n",
    "# incorrect_date_format_df = filtered_alphnum_df[~filtered_alphnum_df['date_of_birth'].apply(has_yyyy_mm_dd_format)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def null_n_nans_pass(df_):\n",
    "    \"\"\"\n",
    "    @desc: this function checks if the input dataframe has any NaNs or Nulls\n",
    "    \"\"\"\n",
    "    if df_.isnull().sum().sum() and df_.isna().sum().sum():\n",
    "        raise f\"The database : {table_name}, has total {df_.isnull().sum().sum()} NULL values and {df_.isna().sum().sum()} NaN values\"\n",
    "    else:\n",
    "        print(f\"[usrmsg] No NULLs or NaNs found in {table_name}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "airdmp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
